# APS Failure Detection â€” Scania Trucks (Challenge@Stellantis)

This repository hosts the **APS (Air Pressure System) Failure Detection** project based on Scania Trucks data.  
The goal is to build a **clean, modular, and reproducible ML codebase** for fault detection and explainability in APS systems.

---

## ğŸ¯ Goal
Ingest â†’ Preprocess â†’ Feature Engineering â†’ Train â†’ Evaluate â†’ Explain  
All within a minimal, Pythonic structure designed for collaborative development and CI/CD integration.

---

## ğŸš€ Quickstart

```bash
# Create a virtual environment
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run tests to confirm setup
pytest -q

# (Optional) Explore notebooks
jupyter notebook notebooks/
```
## ğŸ“¦ Data

Source: Scania CV AB (2016) APS dataset â€” operational data from heavy-truck air-pressure systems.
Summary:

~60,000 training samples (â‰ˆ59k negative, â‰ˆ1k positive)

~16,000 test samples

171 anonymized numeric attributes (sensor counters & histograms)

Missing values marked as na

Target column: class (1 = positive, 0 = negative)

Official cost metric:
Total_cost=10Ã—FP+500Ã—FN

Repository layout for datasets:
```bash
data/
â”œâ”€ raw/              # Original CSVs â€” LFS tracked
â”œâ”€ processed/        # Cleaned/preprocessed datasets â€” LFS tracked
â””â”€ house_processed/  # In-house artifacts (â‰¤ 10 MB): features, models, reports
```

## Notes

Large files (CSV, models >10MB) must be tracked with Git LFS.

Put any generated artifacts from scripts/notebooks under data/house_processed/.

Avoid committing heavy files directly to Git history.

## ğŸ§© Modules

All code lives under src/challenge/ as an installable package.

Module	Purpose
ingest/	Load Scania APS CSVs (aps_failure_training_set.csv, aps_failure_test_set.csv) + missing-value handling.
preprocess/	Cleaning, imputation, scaling, train/test schema consistency.
features/	Feature engineering: meta-features, ratios, PCA transforms.
novelty/	Models & scoring (LogReg, RF, XGBoost, IsolationForest) + cost evaluation utility.
visualize/	EDA helpers: PCA plots, confusion matrices, feature importance.
utils/	Configs, constants, paths, small helpers.

Tests live in src/tests/ with slim placeholders to grow with the project.

## ğŸ§° How To Initialize
```bash
# Clone repository
git clone <your_repo_url>
cd aps-failure-scania

# Initialize Git LFS (once per machine)
git lfs install

# (If needed) Pull large data files
git lfs pull

# Set up environment & dependencies
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Verify installation
pytest -q
```
---

## ğŸ”„ Workflow Overview

Ingest â€“ Load & sanity-check CSVs (types, missingness, target balance).

Preprocess â€“ Impute, scale, and align train/test schemas; manage class imbalance.

Feature Engineering â€“ Derive statistical features & PCA components.

Train â€“ Train baseline & advanced models (LogReg, RF, XGBoost, IsolationForest).

Evaluate â€“ Report Scania cost metric, confusion matrix, precision/recall.

Explain â€“ Use SHAP/feature importance to interpret model behavior.

Test â€“ Grow src/tests/ to keep the pipeline robust and CI-friendly.

## âœ… Continuous Integration

GitHub Actions runs on each push/PR to:

Install dependencies

Run tests (pytest)

Validate imports and basic pipeline execution

Report build status (âœ…/âŒ) on PRs to keep main stable

## ğŸ“ Conventions

Python 3.10+

Keep notebooks light; put core logic in src/

Small, focused commits with clear docstrings

Track large datasets/models via Git LFS

## ğŸ“š Project Notes

Cost-sensitive evaluation is central (high FN penalty vs FP).

Class imbalance requires careful handling (resampling, class weights, threshold tuning).

Keep house_processed/ artifacts small for easy CI and reviews.

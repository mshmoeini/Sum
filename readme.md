# APS Failure Detection ‚Äî Scania Trucks (Challenge@Stellantis)

This repository hosts the **APS (Air Pressure System) Failure Detection** project based on Scania Trucks data.  
The goal is to build a **clean, modular, and reproducible ML codebase** for fault detection and explainability in APS systems.

---

## üéØ Goal
Ingest ‚Üí Preprocess ‚Üí Feature Engineering ‚Üí Train ‚Üí Evaluate ‚Üí Explain  
All within a minimal, Pythonic structure designed for collaborative development and CI/CD integration.

---

## üöÄ Quickstart

```bash
# Create a virtual environment
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run tests to confirm setup
pytest -q

# (Optional) Explore notebooks
jupyter notebook notebooks/

## üöÄ Data

```bash
Source: Scania CV AB (2016) APS dataset ‚Äî operational data from heavy-truck air-pressure systems.
Summary:

~60,000 training samples (‚âà59k negative, ‚âà1k positive)

~16,000 test samples

171 anonymized numeric attributes (sensor counters & histograms)

Missing values marked as na

Target column: class (1 = positive, 0 = negative)

Official cost metric:

Total_cost
=
10
√ó
FP
+
500
√ó
FN
Total_cost=10√óFP+500√óFN

Repository layout for datasets:

data/
‚îú‚îÄ raw/              # Original CSVs ‚Äî LFS tracked
‚îú‚îÄ processed/        # Cleaned/preprocessed datasets ‚Äî LFS tracked
‚îî‚îÄ house_processed/  # In-house artifacts (‚â§ 10 MB): features, models, reports


## üöÄ Notes

```bash

Large files (CSV, models >10MB) must be tracked with Git LFS.

Put any generated artifacts from scripts/notebooks under data/house_processed/.

Avoid committing heavy files directly to Git history.

## Modules

```bash

All code lives under src/challenge/ as an installable package.

Module	Purpose
ingest/	Load Scania APS CSVs (aps_failure_training_set.csv, aps_failure_test_set.csv) + missing-value handling.
preprocess/	Cleaning, imputation, scaling, train/test schema consistency.
features/	Feature engineering: meta-features, ratios, PCA transforms.
novelty/	Models & scoring (LogReg, RF, XGBoost, IsolationForest) + cost evaluation utility.
visualize/	EDA helpers: PCA plots, confusion matrices, feature importance.
utils/	Configs, constants, paths, small helpers.

Tests live in src/tests/ with slim placeholders to grow with the project.


## How To Initialize

```bash
# Clone repository
git clone <your_repo_url>
cd aps-failure-scania

# Initialize Git LFS (once per machine)
git lfs install

# (If needed) Pull large data files
git lfs pull

# Set up environment & dependencies
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Verify installation
pytest -q

## Workflow Overview

Ingest ‚Äì Load & sanity-check CSVs (types, missingness, target balance).

Preprocess ‚Äì Impute, scale, and align train/test schemas; manage class imbalance.

Feature Engineering ‚Äì Derive statistical features & PCA components.

Train ‚Äì Train baseline & advanced models (LogReg, RF, XGBoost, IsolationForest).

Evaluate ‚Äì Report Scania cost metric, confusion matrix, precision/recall.

Explain ‚Äì Use SHAP/feature importance to interpret model behavior.

Test ‚Äì Grow src/tests/ to keep the pipeline robust and CI-friendly.

## Continuous Integration

GitHub Actions runs on each push/PR to:

Install dependencies

Run tests (pytest)

Validate imports and basic pipeline execution

Report build status (‚úÖ/‚ùå) on PRs to keep main stable

## Conventions

Python 3.10+

Keep notebooks light; put core logic in src/

Small, focused commits with clear docstrings

Track large datasets/models via Git LFS

## Project Notes

Cost-sensitive evaluation is central (high FN penalty vs FP).

Class imbalance requires careful handling (resampling, class weights, threshold tuning).

Keep house_processed/ artifacts small for easy CI and reviews.
